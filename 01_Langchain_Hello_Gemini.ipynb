{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Required Libraries\n",
        "Run the following commands to ensure all required libraries are installed:"
      ],
      "metadata": {
        "id": "UR3xrzDgAxZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install python-dotenv  # Optional, for managing API keys\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhEf-DQW6iSu",
        "outputId": "850a1d3b-11af-4035-f0e3-636837cf6d42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Libraries\n",
        "Update your imports to include the correct modules for Google Gemini:"
      ],
      "metadata": {
        "id": "PpF72Vy0AtxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any\n"
      ],
      "metadata": {
        "id": "5j5Ry0sD7Nm8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Set Up the Gemini API\n",
        "Obtain your API key from the Google AI Studio or Google Cloud, then configure the Gemini model:\n"
      ],
      "metadata": {
        "id": "6ARplUYIAo7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gnfgcczT8Zqd",
        "outputId": "a074bea5-2ef7-4e17-eee7-b5fa5e9c3209"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyAvD18bPfhtS3Y0TAvOVXPkkpPP_3eH0UY'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Retrieve the API key securely\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Verify if the key is loaded\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY' is set in userdata.\")\n",
        "\n",
        "# Initialize the Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x1wLCszo7jEG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create a Custom Wrapper for Google Gemini\n",
        "LangChain allows creating a custom LLM class. Hereâ€™s the wrapper for Gemini:"
      ],
      "metadata": {
        "id": "TIeJKOejAdbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    model: str = \"gemini-pro\"  # Model name\n",
        "    temperature: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        model = genai.GenerativeModel(self.model)\n",
        "        response = model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
        "        return response.text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model, \"temperature\": self.temperature}\n"
      ],
      "metadata": {
        "id": "W-upmihz70p9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Integrate with LangChain\n",
        "Now you can use the GeminiLLM class as a replacement for GoogleGeminiFlash:"
      ],
      "metadata": {
        "id": "3X95g6koAXKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini LLM\n",
        "llm = GeminiLLM(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LangChain pipeline\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XABPrb5X73Q1",
        "outputId": "f4bc29cc-2b5f-45cc-d419-44bf77fd828b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: **LangChain** is a large language model developed by Microsoft. It is a transformer-based model, similar to GPT-3 and BLOOM, and is trained on a massive dataset of text and code. LangChain is known for its strong performance on a wide range of natural language processing tasks, including text generation, translation, question answering, and code generation. It is also capable of generating creative text, such as stories and poems. LangChain is used in various applications, including search engines, chatbots, and language translation tools.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Ask a sample question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "n9RBpuON8kE-",
        "outputId": "61a7960c-2976-4b84-94ce-87b72a6ffa00"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a decentralized protocol that allows developers to build and deploy language models on the blockchain. It provides a platform for training, deploying, and consuming language models, enabling developers to create innovative applications that leverage natural language processing (NLP) capabilities. By leveraging the power of blockchain technology, LangChain aims to enhance the security, transparency, and scalability of language models, fostering collaboration and accessibility within the NLP community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "4cpqPPxOFAuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Steps in Colab Project**"
      ],
      "metadata": {
        "id": "P9PForA_E71X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Experiment with Prompts\n",
        "You can create multiple prompt templates to see how the model responds to various formats and tasks."
      ],
      "metadata": {
        "id": "OPK-xaQgBgu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for answering factual questions\n",
        "fact_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a knowledgeable assistant. Answer this factual question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Template for creative storytelling\n",
        "story_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are a creative storyteller. Write a short story about the following topic:\\n{topic}\"\n",
        ")\n",
        "\n",
        "# Using a different template\n",
        "question = \"Tell me a short story about a quaid e azam.\"\n",
        "response = LLMChain(llm=llm, prompt=story_template).run({\"topic\": question})\n",
        "print(\"Story Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "3iXQJD1M_96r",
        "outputId": "a51790d5-3c00-43f3-b749-0c35be9d4be8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Response: In the hallowed halls of history, where the echoes of great minds reverberate through time, the name of Muhammad Ali Jinnah stands tall as a beacon of brilliance and unwavering determination. Known as Quaid-e-Azam, the \"Great Leader,\" his legacy as the founding father of Pakistan is etched in the annals of the nation's soul.\n",
            "\n",
            "One fateful evening, as the sun began its descent over the bustling city of Karachi, a group of young students gathered in the grand auditorium of the University of Karachi. Eager anticipation filled the air as they awaited the arrival of the Quaid-e-Azam.\n",
            "\n",
            "Amidst thunderous applause, a frail but dignified figure emerged from the shadows. Quaid-e-Azam, his countenance etched with wisdom and resolve, stood before the rapt audience. His piercing gaze swept across the room, igniting a spark of inspiration in the hearts of every student present.\n",
            "\n",
            "With a voice that resonated with authority and conviction, he began to speak. His words painted a vivid tapestry of the nation's struggle for independence, a testament to the indomitable spirit of his people. He implored the youth to embrace the mantle of leadership, reminding them that the future of Pakistan lay in their hands.\n",
            "\n",
            "\"You, the youth of today, are the architects of tomorrow's Pakistan,\" he declared. \"Upon your shoulders rests the responsibility to uphold the ideals of our nation: unity, faith, and discipline. Let your actions be guided by the principles of justice, equality, and tolerance.\"\n",
            "\n",
            "As Quaid-e-Azam's speech drew to a close, the auditorium erupted in a standing ovation. The students, their hearts filled with a newfound sense of purpose, vowed to honor his legacy and to strive for a Pakistan that would be a shining beacon of progress and prosperity in the world.\n",
            "\n",
            "In the twilight of his years, Quaid-e-Azam left an indelible mark on the hearts and minds of his people. His words continue to inspire generations of Pakistanis to this day, reminding them of the sacrifices made by their forefathers and the unwavering hope that guided their nation's birth.\n",
            "\n",
            "And so, as the sun set on that historic evening, the legend of Quaid-e-Azam, the Great Leader, was passed down through generations, a testament to the power of one man's unwavering belief in the dreams of a nation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Add Memory (Multi-Turn Conversations)\n",
        "LangChain provides memory for maintaining context across multiple interactions."
      ],
      "metadata": {
        "id": "CHce1kU_Cmjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reuire module installation\n",
        "!pip install langchain-experimental -q\n"
      ],
      "metadata": {
        "id": "zbUhavVOBTOh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Start a conversation\n",
        "response1 = conversation.run(\"What is LangChain?\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = conversation.run(\"Can you explain it in simple words?\")\n",
        "print(\"Q2:\", response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "aeCceKIjC2yP",
        "outputId": "4b026f2d-757d-4c8d-e9a9-ced9f73c0cb7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-0daba4cfee94>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-45-0daba4cfee94>:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: LangChain is a large language model developed by Google. It is a transformer-based model that was trained on a massive dataset of text and code. LangChain is designed to understand and generate human language, and it can be used for a variety of tasks, such as question answering, machine translation, and dialogue generation.\n",
            "\n",
            "Do you have any other questions about LangChain?\n",
            "Q2: LangChain is like a super smart computer program that can understand and talk like a human. It was trained on a huge collection of books, articles, and code, so it knows a lot about the world. LangChain can answer your questions, translate languages, and even write stories. It's like having a really knowledgeable and helpful friend in your computer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMUSpBeEDFMG",
        "outputId": "7e904a95-3275-4cec-b64b-59c4fa6d24ed"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='What is LangChain?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is a large language model developed by Google. It is a transformer-based model that was trained on a massive dataset of text and code. LangChain is designed to understand and generate human language, and it can be used for a variety of tasks, such as question answering, machine translation, and dialogue generation.\\n\\nDo you have any other questions about LangChain?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you explain it in simple words?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"LangChain is like a super smart computer program that can understand and talk like a human. It was trained on a huge collection of books, articles, and code, so it knows a lot about the world. LangChain can answer your questions, translate languages, and even write stories. It's like having a really knowledgeable and helpful friend in your computer!\", additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Explore Gemini Features\n",
        "Fine-tune Gemini responses by adjusting parameters like temperature and max_tokens.\n",
        "\n",
        "Temperature: Controls creativity (higher = more creative).\n",
        "Max Tokens: Limits response length."
      ],
      "metadata": {
        "id": "BcWu0QItDjg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust model settings\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "VQNcbf3IDopG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Combining It All**\n",
        "Combine multiple enhancementsâ€”like memory, new prompts, and toolsâ€”into a full pipeline"
      ],
      "metadata": {
        "id": "izlHtj9GDaV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# configure model\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-pro\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")\n",
        "# Template\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer this question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Chain with memory\n",
        "conversation = LLMChain(llm=llm, prompt=template, memory=memory)\n",
        "\n"
      ],
      "metadata": {
        "id": "gG0jxZh0DQAq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run interactions\n",
        "response1 = conversation.run({\"question\": \"define science?\"})\n",
        "print(response1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Y6qhHoJHELhB",
        "outputId": "8d78f991-3a83-4a8c-cedc-d1da63f6ce0d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Science** is the systematic and organized study of the natural world through observation, testing, and experimentation. It seeks to describe and explain natural phenomena, and to make predictions about the future based on evidence. The goal of science is to gain a better understanding of the universe and our place in it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = conversation.run({\"question\": \"Explain it in a funny way.\"})\n",
        "print(response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "hoUNE77EEiR1",
        "outputId": "c16de892-7083-4356-b27d-a0e35a396196"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine your brain is a wacky rollercoaster, with thoughts zooming around like out-of-control minecarts. When you're multitasking, it's like trying to balance on top of all those minecarts while blindfolded and juggling bowling balls. It's a circus that would make even the clowns dizzy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response3 = conversation.run({\"question\": \"how many branches of science?\"})\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "b2qpFqJ_EkFf",
        "outputId": "336754b2-6d5a-40dc-df55-70a2874b9396"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are numerous branches of science, but some major ones include:\n",
            "\n",
            "**Natural Sciences:**\n",
            "\n",
            "* **Physical Sciences:** Physics, Chemistry, Astronomy, Earth Science\n",
            "* **Life Sciences:** Biology, Zoology, Botany, Ecology\n",
            "* **Formal Sciences:** Mathematics, Logic, Computer Science\n",
            "\n",
            "**Social Sciences:**\n",
            "\n",
            "* Sociology\n",
            "* Psychology\n",
            "* Anthropology\n",
            "* Economics\n",
            "* Political Science\n",
            "* Linguistics\n",
            "\n",
            "**Applied Sciences:**\n",
            "\n",
            "* Engineering\n",
            "* Medicine\n",
            "* Agriculture\n",
            "* Computer Science\n",
            "* Materials Science\n",
            "\n",
            "**Interdisciplinary Sciences:**\n",
            "\n",
            "* Environmental Science\n",
            "* Cognitive Science\n",
            "* Neuroscience\n",
            "* Bioinformatics\n",
            "* Data Science\n",
            "\n",
            "The exact number of branches of science is subjective and can vary depending on classification methods. However, the above list represents the major and commonly recognized fields within the scientific discipline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fts2VarEomh",
        "outputId": "bf5f40e9-16c4-4e6b-e9df-a49f80e152c9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='define science?', additional_kwargs={}, response_metadata={}), AIMessage(content='**Science** is the systematic and organized study of the natural world through observation, testing, and experimentation. It seeks to describe and explain natural phenomena, and to make predictions about the future based on evidence. The goal of science is to gain a better understanding of the universe and our place in it.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain it in a funny way.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Imagine your brain is a wacky rollercoaster, with thoughts zooming around like out-of-control minecarts. When you're multitasking, it's like trying to balance on top of all those minecarts while blindfolded and juggling bowling balls. It's a circus that would make even the clowns dizzy!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='how many branches of science?', additional_kwargs={}, response_metadata={}), AIMessage(content='There are numerous branches of science, but some major ones include:\\n\\n**Natural Sciences:**\\n\\n* **Physical Sciences:** Physics, Chemistry, Astronomy, Earth Science\\n* **Life Sciences:** Biology, Zoology, Botany, Ecology\\n* **Formal Sciences:** Mathematics, Logic, Computer Science\\n\\n**Social Sciences:**\\n\\n* Sociology\\n* Psychology\\n* Anthropology\\n* Economics\\n* Political Science\\n* Linguistics\\n\\n**Applied Sciences:**\\n\\n* Engineering\\n* Medicine\\n* Agriculture\\n* Computer Science\\n* Materials Science\\n\\n**Interdisciplinary Sciences:**\\n\\n* Environmental Science\\n* Cognitive Science\\n* Neuroscience\\n* Bioinformatics\\n* Data Science\\n\\nThe exact number of branches of science is subjective and can vary depending on classification methods. However, the above list represents the major and commonly recognized fields within the scientific discipline.', additional_kwargs={}, response_metadata={})]))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "Y68ASmmEEv02"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rDzz4sGJEyDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}